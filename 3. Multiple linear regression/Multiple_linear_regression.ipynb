{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adfdf05a",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bec3fd",
   "metadata": {},
   "source": [
    "**Regression** models are used to describe relationships between variables by fitting a line to the observed data. Regression allows you to estimate how a dependent variable changes as the independent variable(s) change.\n",
    "\n",
    "**Multiple linear regression** is used to estimate the relationship between two or more independent variables and one dependent variable. You can use multiple linear regression when you want to know:\n",
    "\n",
    "1. How strong the relationship is between two or more independent variables and one dependent variable (e.g. how rainfall, temperature, and amount of fertilizer added affect crop growth).\n",
    "2. The value of the dependent variable at a certain value of the independent variables (e.g. the expected yield of a crop at certain levels of rainfall, temperature, and fertilizer addition).\n",
    "\n",
    "**Assumptions of multiple linear regression**\n",
    "\n",
    "Multiple linear regression makes all of the same assumptions as simple linear regression:\n",
    "\n",
    "1. Homogeneity of variance (homoscedasticity): the size of the error in our prediction doesn’t change significantly across the values of the independent variable.\n",
    "\n",
    "    Homogeneity of variance, also known as homoscedasticity, is a statistical concept that refers to the assumption that the variance of the errors or residuals in a statistical model is constant across all levels of the independent variable(s). In simpler terms, it means that the spread of the data points around the mean is roughly the same across all groups or conditions being compared.\n",
    "\n",
    "    This is an important assumption of parametric statistical tests because they are sensitive to any dissimilarities. Uneven variances in samples result in biased and skewed test results.\n",
    "\n",
    "    If the variances are not equal, then the standard errors, confidence intervals, and p-values will be affected, and the conclusions drawn from the analysis may not be valid.\n",
    "\n",
    "    There are various methods to test for homogeneity of variance, including graphical methods such as scatterplots and boxplots\n",
    "\n",
    "2. Independence of observations: the observations in the dataset were collected using statistically valid sampling methods, and there are no hidden relationships among variables.\n",
    "\n",
    "    In multiple linear regression, it is possible that some of the independent variables are actually correlated with one another, so it is important to check these before developing the regression model. If two independent variables are too highly correlated (r2 > ~0.6), then only one of them should be used in the regression model.\n",
    "\n",
    "3. Normality: The data follows a normal distribution.\n",
    "\n",
    "4. Linearity: the line of best fit through the data points is a straight line, rather than a curve or some sort of grouping factor.\n",
    "\n",
    "The **general equation** for a multiple linear regression model is:\n",
    "\n",
    "y = β0 + β1x1 + β2x2 + ... + βpxp + ε\n",
    "\n",
    "Where:\n",
    "\n",
    "y is the dependent variable\n",
    "β0 is the intercept or constant term\n",
    "β1, β2, ..., βp are the coefficients or regression weights for the independent variables x1, x2, ..., xp, respectively\n",
    "ε is the error term, which represents the random variation or unexplained factors affecting the dependent variable\n",
    "The goal of multiple linear regression is to estimate the values of the regression coefficients that best fit the data and allow us to make predictions about the dependent variable for new values of the independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a473ed76",
   "metadata": {},
   "source": [
    "**Multicollinearity** refers to a situation in multiple linear regression where two or more independent variables in the model are highly correlated with each other. This can lead to problems in the estimation of the regression coefficients, making them unstable or difficult to interpret.\n",
    "\n",
    "Multicollinearity can arise for several reasons, including:\n",
    "\n",
    "1.  Including redundant or highly similar variables in the model\n",
    "2. Measuring the same variable using different units or scales\n",
    "3. Overfitting the model to the data, i.e., including too many independent variables relative to the sample size\n",
    "\n",
    "\n",
    "The **dummy variable trap** is a common problem that can occur in multiple linear regression models when one or more categorical variables are included as independent variables. It occurs when we include a categorical variable in the regression model using a set of dummy variables, but we do not exclude one of the dummy variables from the regression equation.\n",
    "\n",
    "For example, suppose we have a categorical variable \"Region\" with three possible values: East, West, and South. We could create two dummy variables, \"West\" and \"South,\" to represent the West and South regions, respectively. However, if we include both dummy variables in the regression model, the model will suffer from the dummy variable trap because the intercept term and the coefficients for each dummy variable will be highly correlated, making it difficult to estimate the true effects of the independent variables on the dependent variable.\n",
    "\n",
    "\n",
    "To avoid the dummy variable trap, we need to exclude one of the dummy variables from the regression equation, which can be done by either dropping one of the categories (in this example, East) or by combining the dummy variables into a single variable that represents the omitted category. This approach is known as the \"reference category\" or \"baseline category\" method.\n",
    "\n",
    "In summary, the dummy variable trap occurs when we include all dummy variables for a categorical variable in the regression equation, and it can be avoided by excluding one of the dummy variables or combining them into a single variable. By avoiding the trap, we can obtain more accurate estimates of the regression coefficients and make more reliable predictions using the model.\n",
    "\n",
    "The \"dummy variable trap\" is a term used in statistics and regression analysis. It refers to a situation where a set of predictor variables (also known as independent variables or features) in a regression model are highly correlated with each other. Specifically, the dummy variable trap occurs when two or more dummy variables are perfectly collinear, meaning they can be predicted perfectly from each other.\n",
    "\n",
    "In regression analysis, dummy variables are used to represent categorical data as numerical values. For example, a dummy variable could be used to represent a categorical variable such as gender, with 1 indicating male and 0 indicating female. However, if a regression model includes dummy variables for both gender and, say, marital status, there may be a high correlation between the two variables. This can lead to problems in the regression model, including biased coefficient estimates and inflated standard errors.\n",
    "\n",
    "To avoid the dummy variable trap, one of the dummy variables must be dropped from the regression model. This is typically done by selecting a reference category and dropping the dummy variable for that category. For example, if gender and marital status are included in a regression model, the dummy variable for one gender or one marital status (usually the most common one) can be dropped from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18913d53",
   "metadata": {},
   "source": [
    "### Multiple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "430838ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R&amp;D Spend</th>\n",
       "      <th>Administration</th>\n",
       "      <th>Marketing Spend</th>\n",
       "      <th>State</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>165349.20</td>\n",
       "      <td>136897.80</td>\n",
       "      <td>471784.10</td>\n",
       "      <td>New York</td>\n",
       "      <td>192261.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>162597.70</td>\n",
       "      <td>151377.59</td>\n",
       "      <td>443898.53</td>\n",
       "      <td>California</td>\n",
       "      <td>191792.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>153441.51</td>\n",
       "      <td>101145.55</td>\n",
       "      <td>407934.54</td>\n",
       "      <td>Florida</td>\n",
       "      <td>191050.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>144372.41</td>\n",
       "      <td>118671.85</td>\n",
       "      <td>383199.62</td>\n",
       "      <td>New York</td>\n",
       "      <td>182901.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>142107.34</td>\n",
       "      <td>91391.77</td>\n",
       "      <td>366168.42</td>\n",
       "      <td>Florida</td>\n",
       "      <td>166187.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   R&D Spend  Administration  Marketing Spend       State     Profit\n",
       "0  165349.20       136897.80        471784.10    New York  192261.83\n",
       "1  162597.70       151377.59        443898.53  California  191792.06\n",
       "2  153441.51       101145.55        407934.54     Florida  191050.39\n",
       "3  144372.41       118671.85        383199.62    New York  182901.99\n",
       "4  142107.34        91391.77        366168.42     Florida  166187.94"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#importing dataset\n",
    "dataset = pd.read_csv(\"50_Startups.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a87993ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb416b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[192261.83 191792.06 191050.39 182901.99 166187.94 156991.12 156122.51\n",
      " 155752.6  152211.77 149759.96 146121.95 144259.4  141585.52 134307.35\n",
      " 132602.65 129917.04 126992.93 125370.37 124266.9  122776.86 118474.03\n",
      " 111313.02 110352.25 108733.99 108552.04 107404.34 105733.54 105008.31\n",
      " 103282.38 101004.64  99937.59  97483.56  97427.84  96778.92  96712.8\n",
      "  96479.51  90708.19  89949.14  81229.06  81005.76  78239.91  77798.83\n",
      "  71498.49  69758.98  65200.33  64926.08  49490.75  42559.73  35673.41\n",
      "  14681.4 ]\n"
     ]
    }
   ],
   "source": [
    "X  = dataset.iloc[:,:-1].values\n",
    "y = dataset.loc[:,'Profit'].values\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1eec116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[165349.2, 136897.8, 471784.1, 2],\n",
       "       [162597.7, 151377.59, 443898.53, 0],\n",
       "       [153441.51, 101145.55, 407934.54, 1],\n",
       "       [144372.41, 118671.85, 383199.62, 2],\n",
       "       [142107.34, 91391.77, 366168.42, 1],\n",
       "       [131876.9, 99814.71, 362861.36, 2],\n",
       "       [134615.46, 147198.87, 127716.82, 0],\n",
       "       [130298.13, 145530.06, 323876.68, 1],\n",
       "       [120542.52, 148718.95, 311613.29, 2],\n",
       "       [123334.88, 108679.17, 304981.62, 0],\n",
       "       [101913.08, 110594.11, 229160.95, 1],\n",
       "       [100671.96, 91790.61, 249744.55, 0],\n",
       "       [93863.75, 127320.38, 249839.44, 1],\n",
       "       [91992.39, 135495.07, 252664.93, 0],\n",
       "       [119943.24, 156547.42, 256512.92, 1],\n",
       "       [114523.61, 122616.84, 261776.23, 2],\n",
       "       [78013.11, 121597.55, 264346.06, 0],\n",
       "       [94657.16, 145077.58, 282574.31, 2],\n",
       "       [91749.16, 114175.79, 294919.57, 1],\n",
       "       [86419.7, 153514.11, 0.0, 2],\n",
       "       [76253.86, 113867.3, 298664.47, 0],\n",
       "       [78389.47, 153773.43, 299737.29, 2],\n",
       "       [73994.56, 122782.75, 303319.26, 1],\n",
       "       [67532.53, 105751.03, 304768.73, 1],\n",
       "       [77044.01, 99281.34, 140574.81, 2],\n",
       "       [64664.71, 139553.16, 137962.62, 0],\n",
       "       [75328.87, 144135.98, 134050.07, 1],\n",
       "       [72107.6, 127864.55, 353183.81, 2],\n",
       "       [66051.52, 182645.56, 118148.2, 1],\n",
       "       [65605.48, 153032.06, 107138.38, 2],\n",
       "       [61994.48, 115641.28, 91131.24, 1],\n",
       "       [61136.38, 152701.92, 88218.23, 2],\n",
       "       [63408.86, 129219.61, 46085.25, 0],\n",
       "       [55493.95, 103057.49, 214634.81, 1],\n",
       "       [46426.07, 157693.92, 210797.67, 0],\n",
       "       [46014.02, 85047.44, 205517.64, 2],\n",
       "       [28663.76, 127056.21, 201126.82, 1],\n",
       "       [44069.95, 51283.14, 197029.42, 0],\n",
       "       [20229.59, 65947.93, 185265.1, 2],\n",
       "       [38558.51, 82982.09, 174999.3, 0],\n",
       "       [28754.33, 118546.05, 172795.67, 0],\n",
       "       [27892.92, 84710.77, 164470.71, 1],\n",
       "       [23640.93, 96189.63, 148001.11, 0],\n",
       "       [15505.73, 127382.3, 35534.17, 2],\n",
       "       [22177.74, 154806.14, 28334.72, 0],\n",
       "       [1000.23, 124153.04, 1903.93, 2],\n",
       "       [1315.46, 115816.21, 297114.46, 1],\n",
       "       [0.0, 135426.92, 0.0, 0],\n",
       "       [542.05, 51743.15, 0.0, 2],\n",
       "       [0.0, 116983.8, 45173.06, 0]], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labelencoding for features\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder_X = LabelEncoder()\n",
    "X[:,3] = labelencoder_X.fit_transform(X[:,3])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb63ed8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 1.0 165349.2 136897.8 471784.1]\n",
      " [1.0 0.0 0.0 162597.7 151377.59 443898.53]\n",
      " [0.0 1.0 0.0 153441.51 101145.55 407934.54]\n",
      " [0.0 0.0 1.0 144372.41 118671.85 383199.62]\n",
      " [0.0 1.0 0.0 142107.34 91391.77 366168.42]\n",
      " [0.0 0.0 1.0 131876.9 99814.71 362861.36]\n",
      " [1.0 0.0 0.0 134615.46 147198.87 127716.82]\n",
      " [0.0 1.0 0.0 130298.13 145530.06 323876.68]\n",
      " [0.0 0.0 1.0 120542.52 148718.95 311613.29]\n",
      " [1.0 0.0 0.0 123334.88 108679.17 304981.62]\n",
      " [0.0 1.0 0.0 101913.08 110594.11 229160.95]\n",
      " [1.0 0.0 0.0 100671.96 91790.61 249744.55]\n",
      " [0.0 1.0 0.0 93863.75 127320.38 249839.44]\n",
      " [1.0 0.0 0.0 91992.39 135495.07 252664.93]\n",
      " [0.0 1.0 0.0 119943.24 156547.42 256512.92]\n",
      " [0.0 0.0 1.0 114523.61 122616.84 261776.23]\n",
      " [1.0 0.0 0.0 78013.11 121597.55 264346.06]\n",
      " [0.0 0.0 1.0 94657.16 145077.58 282574.31]\n",
      " [0.0 1.0 0.0 91749.16 114175.79 294919.57]\n",
      " [0.0 0.0 1.0 86419.7 153514.11 0.0]\n",
      " [1.0 0.0 0.0 76253.86 113867.3 298664.47]\n",
      " [0.0 0.0 1.0 78389.47 153773.43 299737.29]\n",
      " [0.0 1.0 0.0 73994.56 122782.75 303319.26]\n",
      " [0.0 1.0 0.0 67532.53 105751.03 304768.73]\n",
      " [0.0 0.0 1.0 77044.01 99281.34 140574.81]\n",
      " [1.0 0.0 0.0 64664.71 139553.16 137962.62]\n",
      " [0.0 1.0 0.0 75328.87 144135.98 134050.07]\n",
      " [0.0 0.0 1.0 72107.6 127864.55 353183.81]\n",
      " [0.0 1.0 0.0 66051.52 182645.56 118148.2]\n",
      " [0.0 0.0 1.0 65605.48 153032.06 107138.38]\n",
      " [0.0 1.0 0.0 61994.48 115641.28 91131.24]\n",
      " [0.0 0.0 1.0 61136.38 152701.92 88218.23]\n",
      " [1.0 0.0 0.0 63408.86 129219.61 46085.25]\n",
      " [0.0 1.0 0.0 55493.95 103057.49 214634.81]\n",
      " [1.0 0.0 0.0 46426.07 157693.92 210797.67]\n",
      " [0.0 0.0 1.0 46014.02 85047.44 205517.64]\n",
      " [0.0 1.0 0.0 28663.76 127056.21 201126.82]\n",
      " [1.0 0.0 0.0 44069.95 51283.14 197029.42]\n",
      " [0.0 0.0 1.0 20229.59 65947.93 185265.1]\n",
      " [1.0 0.0 0.0 38558.51 82982.09 174999.3]\n",
      " [1.0 0.0 0.0 28754.33 118546.05 172795.67]\n",
      " [0.0 1.0 0.0 27892.92 84710.77 164470.71]\n",
      " [1.0 0.0 0.0 23640.93 96189.63 148001.11]\n",
      " [0.0 0.0 1.0 15505.73 127382.3 35534.17]\n",
      " [1.0 0.0 0.0 22177.74 154806.14 28334.72]\n",
      " [0.0 0.0 1.0 1000.23 124153.04 1903.93]\n",
      " [0.0 1.0 0.0 1315.46 115816.21 297114.46]\n",
      " [1.0 0.0 0.0 0.0 135426.92 0.0]\n",
      " [0.0 0.0 1.0 542.05 51743.15 0.0]\n",
      " [1.0 0.0 0.0 0.0 116983.8 45173.06]]\n"
     ]
    }
   ],
   "source": [
    "# Dummy encoding\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "columnTransform = make_column_transformer((OneHotEncoder(categories = 'auto'),[3]),remainder ='passthrough')\n",
    "X = columnTransform.fit_transform(X)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf7ebaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0635cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoiding dummy variable trap\n",
    "X = X[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f09469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40b7b5e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting Multiple Linear Regression to training set\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6222a2f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-9.59284160e+02,  6.99369053e+02,  7.73467193e-01,  3.28845975e-02,\n",
       "        3.66100259e-02])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.coef_"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7e894651",
   "metadata": {},
   "source": [
    "regressor.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2147b5a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42554.16761776563"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d739015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([103015.20159796, 132582.27760816, 132447.73845175,  71976.09851259,\n",
       "       178537.48221054, 116161.24230163,  67851.69209676,  98791.73374688,\n",
       "       113969.43533012, 167921.0656955 ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicting the test set results\n",
    "y_pred = regressor.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56f8f337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predict</th>\n",
       "      <th>Values</th>\n",
       "      <th>Difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103015.201598</td>\n",
       "      <td>103282.38</td>\n",
       "      <td>-267.178402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>132582.277608</td>\n",
       "      <td>144259.40</td>\n",
       "      <td>-11677.122392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>132447.738452</td>\n",
       "      <td>146121.95</td>\n",
       "      <td>-13674.211548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71976.098513</td>\n",
       "      <td>77798.83</td>\n",
       "      <td>-5822.731487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>178537.482211</td>\n",
       "      <td>191050.39</td>\n",
       "      <td>-12512.907789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>116161.242302</td>\n",
       "      <td>105008.31</td>\n",
       "      <td>11152.932302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>67851.692097</td>\n",
       "      <td>81229.06</td>\n",
       "      <td>-13377.367903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>98791.733747</td>\n",
       "      <td>97483.56</td>\n",
       "      <td>1308.173747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>113969.435330</td>\n",
       "      <td>110352.25</td>\n",
       "      <td>3617.185330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>167921.065696</td>\n",
       "      <td>166187.94</td>\n",
       "      <td>1733.125696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Predict     Values    Difference\n",
       "0  103015.201598  103282.38   -267.178402\n",
       "1  132582.277608  144259.40 -11677.122392\n",
       "2  132447.738452  146121.95 -13674.211548\n",
       "3   71976.098513   77798.83  -5822.731487\n",
       "4  178537.482211  191050.39 -12512.907789\n",
       "5  116161.242302  105008.31  11152.932302\n",
       "6   67851.692097   81229.06 -13377.367903\n",
       "7   98791.733747   97483.56   1308.173747\n",
       "8  113969.435330  110352.25   3617.185330\n",
       "9  167921.065696  166187.94   1733.125696"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff = pd.DataFrame({\"Predict\":y_pred,\"Values\":y_test , 'Difference':y_pred-y_test})\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "375636df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83502864.03252874"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "metrics.mean_squared_error(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "492ea26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Standard scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0cdc3001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.73205081, -0.73379939, -0.35006454, -0.78547109,  0.1011968 ],\n",
       "       [-0.57735027,  1.36277029, -0.55530319, -1.48117426,  0.02734979],\n",
       "       [ 1.73205081, -0.73379939,  0.07935762,  0.80133381, -0.55152132],\n",
       "       [-0.57735027, -0.73379939, -0.54638238,  1.32505817,  0.07011684],\n",
       "       [ 1.73205081, -0.73379939,  0.43485371, -0.35598663,  0.75148516],\n",
       "       [ 1.73205081, -0.73379939,  1.26943143,  0.85518519,  0.98603118],\n",
       "       [ 1.73205081, -0.73379939,  1.04525007,  1.28077047,  0.4404    ],\n",
       "       [-0.57735027,  1.36277029, -1.529843  ,  0.02942065, -1.6218751 ],\n",
       "       [-0.57735027,  1.36277029, -1.53976251, -2.76767264, -1.6372965 ],\n",
       "       [-0.57735027,  1.36277029, -0.13115188,  1.14497701, -0.76949991],\n",
       "       [-0.57735027,  1.36277029,  0.92791613, -0.02992062,  0.48303162],\n",
       "       [ 1.73205081, -0.73379939, -0.20932933, -0.2993768 , -0.89915412],\n",
       "       [-0.57735027, -0.73379939, -0.17870828,  0.2251352 , -1.26401642],\n",
       "       [-0.57735027, -0.73379939,  0.1374709 , -0.06929437,  0.50384666],\n",
       "       [-0.57735027, -0.73379939, -1.03967624, -1.05076697, -0.43852106],\n",
       "       [-0.57735027, -0.73379939,  0.09938348, -0.36790317,  0.781818  ],\n",
       "       [-0.57735027,  1.36277029, -1.21580174,  0.15416247, -1.34947778],\n",
       "       [-0.57735027,  1.36277029,  1.05822437,  0.97836757,  0.88670051],\n",
       "       [-0.57735027, -0.73379939,  0.4401196 ,  0.46754749,  0.40923215],\n",
       "       [-0.57735027, -0.73379939, -0.15151937,  0.62430586, -0.51983056],\n",
       "       [-0.57735027,  1.36277029,  1.30361149, -0.91073517,  1.30179825],\n",
       "       [-0.57735027,  1.36277029,  0.49781135,  0.83770651,  0.65149135],\n",
       "       [-0.57735027, -0.73379939, -0.92897212, -0.18716957, -0.23769075],\n",
       "       [-0.57735027, -0.73379939, -1.55149779, -0.24751712, -1.27140496],\n",
       "       [-0.57735027, -0.73379939,  1.96871085,  1.08106713,  1.95818096],\n",
       "       [ 1.73205081, -0.73379939,  0.48063418,  0.15177059,  0.38634632],\n",
       "       [-0.57735027, -0.73379939, -0.59739193, -2.78544219, -0.04140287],\n",
       "       [-0.57735027,  1.36277029,  0.11649007, -0.93133851, -0.49867241],\n",
       "       [-0.57735027, -0.73379939,  1.36290079,  0.91964899, -0.60281921],\n",
       "       [ 1.73205081, -0.73379939, -0.08943162, -0.68142339,  0.83126112],\n",
       "       [ 1.73205081, -0.73379939, -0.93093295,  0.14156607, -0.00821485],\n",
       "       [-0.57735027,  1.36277029,  0.14561902,  1.1736151 ,  0.7905076 ],\n",
       "       [-0.57735027,  1.36277029,  0.31947194,  1.16359793, -1.6372965 ],\n",
       "       [-0.57735027, -0.73379939,  1.11867842, -0.56831342,  0.83298548],\n",
       "       [-0.57735027, -0.73379939, -0.71671353, -1.56095586, -0.21984184],\n",
       "       [ 1.73205081, -0.73379939, -1.52301833, -0.29261949,  0.76926327],\n",
       "       [-0.57735027,  1.36277029,  1.57413686, -0.18231009,  1.46653355],\n",
       "       [-0.57735027,  1.36277029,  2.02828029,  0.52173299,  2.18404776],\n",
       "       [-0.57735027, -0.73379939, -1.55149779,  0.46491495, -1.6372965 ],\n",
       "       [-0.57735027, -0.73379939, -1.07135402,  1.21350725, -1.40779169]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_regressor = LinearRegression()\n",
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8e4623ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_regressor.fit(X_train_scaled,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c56491f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = scaled_regressor.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "84aee60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([103015.20159796, 132582.27760816, 132447.73845174,  71976.09851258,\n",
       "       178537.48221055, 116161.24230165,  67851.69209676,  98791.73374687,\n",
       "       113969.43533012, 167921.0656955 ])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "86c63d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83502864.03257717"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "metrics.mean_squared_error(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a267e5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88903501",
   "metadata": {},
   "source": [
    "#When we take step back do you think it is optimal model that we can meake with dataset we have here because when we built this model we used all the indepedent variable but what \n",
    "if among these model their is highly statistically significant  dependent variable that is had a great impact on the dependent variable profit and some are not significantlly at all\n",
    "that is if =we remove not significantlly statiistcally variable from the model we still get some amazing prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba6babe",
   "metadata": {},
   "source": [
    "###  Backward elimination method\n",
    "\n",
    "Backward elimination is a method used in regression analysis to select the most significant independent variables (predictor variables) for inclusion in a regression model. The backward elimination method involves starting with a model that includes all the independent variables, and then systematically removing variables until only the most significant ones remain.\n",
    "\n",
    "The backward elimination method involves the following steps:\n",
    "\n",
    "Start with a model that includes all independent variables.\n",
    "Evaluate the significance of each independent variable using a statistical test, such as the F-test or t-test. The least significant variable (i.e., the one with the highest p-value) is removed from the model.\n",
    "Re-evaluate the remaining independent variables and repeat step 2 until all remaining variables are statistically significant.\n",
    "This process continues until only the statistically significant independent variables remain in the model. The final model is considered the \"best\" model, as it includes only the most significant independent variables.\n",
    "\n",
    "The backward elimination method can help to simplify a regression model by removing unnecessary variables and reducing the risk of overfitting (i.e., the model fitting too closely to the training data and performing poorly on new data). However, it is important to note that the backward elimination method may not always result in the best model, as the significance of independent variables can be influenced by other variables in the model.\n",
    "\n",
    "\n",
    "Algorithm:\n",
    "\n",
    "Step 1: Select a significance level to stay in the model( eg:SL = 0.05)\n",
    "\n",
    "Step 2: Fit the full model with all possible predictors\n",
    "\n",
    "step 3: Consider the predictor with the highest P=value . If P> SL, go to Step 4 go to finish\n",
    "\n",
    "step 4: Remove the predictor \n",
    "\n",
    "Step 5: Fit model without this variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d6355528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 1.0 165349.2 136897.8 471784.1]\n",
      " [1.0 0.0 0.0 162597.7 151377.59 443898.53]\n",
      " [1.0 1.0 0.0 153441.51 101145.55 407934.54]\n",
      " [1.0 0.0 1.0 144372.41 118671.85 383199.62]\n",
      " [1.0 1.0 0.0 142107.34 91391.77 366168.42]\n",
      " [1.0 0.0 1.0 131876.9 99814.71 362861.36]\n",
      " [1.0 0.0 0.0 134615.46 147198.87 127716.82]\n",
      " [1.0 1.0 0.0 130298.13 145530.06 323876.68]\n",
      " [1.0 0.0 1.0 120542.52 148718.95 311613.29]\n",
      " [1.0 0.0 0.0 123334.88 108679.17 304981.62]\n",
      " [1.0 1.0 0.0 101913.08 110594.11 229160.95]\n",
      " [1.0 0.0 0.0 100671.96 91790.61 249744.55]\n",
      " [1.0 1.0 0.0 93863.75 127320.38 249839.44]\n",
      " [1.0 0.0 0.0 91992.39 135495.07 252664.93]\n",
      " [1.0 1.0 0.0 119943.24 156547.42 256512.92]\n",
      " [1.0 0.0 1.0 114523.61 122616.84 261776.23]\n",
      " [1.0 0.0 0.0 78013.11 121597.55 264346.06]\n",
      " [1.0 0.0 1.0 94657.16 145077.58 282574.31]\n",
      " [1.0 1.0 0.0 91749.16 114175.79 294919.57]\n",
      " [1.0 0.0 1.0 86419.7 153514.11 0.0]\n",
      " [1.0 0.0 0.0 76253.86 113867.3 298664.47]\n",
      " [1.0 0.0 1.0 78389.47 153773.43 299737.29]\n",
      " [1.0 1.0 0.0 73994.56 122782.75 303319.26]\n",
      " [1.0 1.0 0.0 67532.53 105751.03 304768.73]\n",
      " [1.0 0.0 1.0 77044.01 99281.34 140574.81]\n",
      " [1.0 0.0 0.0 64664.71 139553.16 137962.62]\n",
      " [1.0 1.0 0.0 75328.87 144135.98 134050.07]\n",
      " [1.0 0.0 1.0 72107.6 127864.55 353183.81]\n",
      " [1.0 1.0 0.0 66051.52 182645.56 118148.2]\n",
      " [1.0 0.0 1.0 65605.48 153032.06 107138.38]\n",
      " [1.0 1.0 0.0 61994.48 115641.28 91131.24]\n",
      " [1.0 0.0 1.0 61136.38 152701.92 88218.23]\n",
      " [1.0 0.0 0.0 63408.86 129219.61 46085.25]\n",
      " [1.0 1.0 0.0 55493.95 103057.49 214634.81]\n",
      " [1.0 0.0 0.0 46426.07 157693.92 210797.67]\n",
      " [1.0 0.0 1.0 46014.02 85047.44 205517.64]\n",
      " [1.0 1.0 0.0 28663.76 127056.21 201126.82]\n",
      " [1.0 0.0 0.0 44069.95 51283.14 197029.42]\n",
      " [1.0 0.0 1.0 20229.59 65947.93 185265.1]\n",
      " [1.0 0.0 0.0 38558.51 82982.09 174999.3]\n",
      " [1.0 0.0 0.0 28754.33 118546.05 172795.67]\n",
      " [1.0 1.0 0.0 27892.92 84710.77 164470.71]\n",
      " [1.0 0.0 0.0 23640.93 96189.63 148001.11]\n",
      " [1.0 0.0 1.0 15505.73 127382.3 35534.17]\n",
      " [1.0 0.0 0.0 22177.74 154806.14 28334.72]\n",
      " [1.0 0.0 1.0 1000.23 124153.04 1903.93]\n",
      " [1.0 1.0 0.0 1315.46 115816.21 297114.46]\n",
      " [1.0 0.0 0.0 0.0 135426.92 0.0]\n",
      " [1.0 0.0 1.0 542.05 51743.15 0.0]\n",
      " [1.0 0.0 0.0 0.0 116983.8 45173.06]]\n"
     ]
    }
   ],
   "source": [
    "# building an optimial model using backward elimination method \n",
    "#  as equation of multiple linear regression is y = b0+ b1*x1 + b2*X2 + ..+ bn*xn\n",
    "# where y is dependent variable , b0 is constant , x1,x2.. are independent variable and b1,b2,b3.... are coefficients of independent variable\n",
    "# as b0 is not associated with independent var. x but it is as x0 is equal to 1\n",
    "# the model we use to use it right now it dosent take b0 but we will make enviornment to add it\n",
    "import statsmodels.api as sm \n",
    "print(X)\n",
    "X = np.append(arr=np.ones([50,1]),values = X,axis = 1)  # we added the b0 , adding to column\n",
    "# but we have to add this colunm at first we will add  X to ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a97232e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0, 1.0, 0.0, 1.0, 165349.2, 136897.8, 471784.1],\n",
       "       [1.0, 1.0, 0.0, 0.0, 162597.7, 151377.59, 443898.53],\n",
       "       [1.0, 1.0, 1.0, 0.0, 153441.51, 101145.55, 407934.54],\n",
       "       [1.0, 1.0, 0.0, 1.0, 144372.41, 118671.85, 383199.62],\n",
       "       [1.0, 1.0, 1.0, 0.0, 142107.34, 91391.77, 366168.42],\n",
       "       [1.0, 1.0, 0.0, 1.0, 131876.9, 99814.71, 362861.36],\n",
       "       [1.0, 1.0, 0.0, 0.0, 134615.46, 147198.87, 127716.82],\n",
       "       [1.0, 1.0, 1.0, 0.0, 130298.13, 145530.06, 323876.68],\n",
       "       [1.0, 1.0, 0.0, 1.0, 120542.52, 148718.95, 311613.29],\n",
       "       [1.0, 1.0, 0.0, 0.0, 123334.88, 108679.17, 304981.62],\n",
       "       [1.0, 1.0, 1.0, 0.0, 101913.08, 110594.11, 229160.95],\n",
       "       [1.0, 1.0, 0.0, 0.0, 100671.96, 91790.61, 249744.55],\n",
       "       [1.0, 1.0, 1.0, 0.0, 93863.75, 127320.38, 249839.44],\n",
       "       [1.0, 1.0, 0.0, 0.0, 91992.39, 135495.07, 252664.93],\n",
       "       [1.0, 1.0, 1.0, 0.0, 119943.24, 156547.42, 256512.92],\n",
       "       [1.0, 1.0, 0.0, 1.0, 114523.61, 122616.84, 261776.23],\n",
       "       [1.0, 1.0, 0.0, 0.0, 78013.11, 121597.55, 264346.06],\n",
       "       [1.0, 1.0, 0.0, 1.0, 94657.16, 145077.58, 282574.31],\n",
       "       [1.0, 1.0, 1.0, 0.0, 91749.16, 114175.79, 294919.57],\n",
       "       [1.0, 1.0, 0.0, 1.0, 86419.7, 153514.11, 0.0],\n",
       "       [1.0, 1.0, 0.0, 0.0, 76253.86, 113867.3, 298664.47],\n",
       "       [1.0, 1.0, 0.0, 1.0, 78389.47, 153773.43, 299737.29],\n",
       "       [1.0, 1.0, 1.0, 0.0, 73994.56, 122782.75, 303319.26],\n",
       "       [1.0, 1.0, 1.0, 0.0, 67532.53, 105751.03, 304768.73],\n",
       "       [1.0, 1.0, 0.0, 1.0, 77044.01, 99281.34, 140574.81],\n",
       "       [1.0, 1.0, 0.0, 0.0, 64664.71, 139553.16, 137962.62],\n",
       "       [1.0, 1.0, 1.0, 0.0, 75328.87, 144135.98, 134050.07],\n",
       "       [1.0, 1.0, 0.0, 1.0, 72107.6, 127864.55, 353183.81],\n",
       "       [1.0, 1.0, 1.0, 0.0, 66051.52, 182645.56, 118148.2],\n",
       "       [1.0, 1.0, 0.0, 1.0, 65605.48, 153032.06, 107138.38],\n",
       "       [1.0, 1.0, 1.0, 0.0, 61994.48, 115641.28, 91131.24],\n",
       "       [1.0, 1.0, 0.0, 1.0, 61136.38, 152701.92, 88218.23],\n",
       "       [1.0, 1.0, 0.0, 0.0, 63408.86, 129219.61, 46085.25],\n",
       "       [1.0, 1.0, 1.0, 0.0, 55493.95, 103057.49, 214634.81],\n",
       "       [1.0, 1.0, 0.0, 0.0, 46426.07, 157693.92, 210797.67],\n",
       "       [1.0, 1.0, 0.0, 1.0, 46014.02, 85047.44, 205517.64],\n",
       "       [1.0, 1.0, 1.0, 0.0, 28663.76, 127056.21, 201126.82],\n",
       "       [1.0, 1.0, 0.0, 0.0, 44069.95, 51283.14, 197029.42],\n",
       "       [1.0, 1.0, 0.0, 1.0, 20229.59, 65947.93, 185265.1],\n",
       "       [1.0, 1.0, 0.0, 0.0, 38558.51, 82982.09, 174999.3],\n",
       "       [1.0, 1.0, 0.0, 0.0, 28754.33, 118546.05, 172795.67],\n",
       "       [1.0, 1.0, 1.0, 0.0, 27892.92, 84710.77, 164470.71],\n",
       "       [1.0, 1.0, 0.0, 0.0, 23640.93, 96189.63, 148001.11],\n",
       "       [1.0, 1.0, 0.0, 1.0, 15505.73, 127382.3, 35534.17],\n",
       "       [1.0, 1.0, 0.0, 0.0, 22177.74, 154806.14, 28334.72],\n",
       "       [1.0, 1.0, 0.0, 1.0, 1000.23, 124153.04, 1903.93],\n",
       "       [1.0, 1.0, 1.0, 0.0, 1315.46, 115816.21, 297114.46],\n",
       "       [1.0, 1.0, 0.0, 0.0, 0.0, 135426.92, 0.0],\n",
       "       [1.0, 1.0, 0.0, 1.0, 542.05, 51743.15, 0.0],\n",
       "       [1.0, 1.0, 0.0, 0.0, 0.0, 116983.8, 45173.06]], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64971f0f",
   "metadata": {},
   "source": [
    "Ordinary Least Squares (OLS) is a method used in linear regression analysis to estimate the parameters of a linear regression model. The OLS algorithm involves finding the values of the regression coefficients that minimize the sum of the squared residuals between the predicted values and the actual values of the dependent variable.\n",
    "\n",
    "The OLS algorithm involves the following steps:\n",
    "\n",
    "Define the linear regression model: The linear regression model is defined as a linear combination of the independent variables (predictors) and the regression coefficients.\n",
    "\n",
    "Estimate the regression coefficients: The regression coefficients are estimated using the OLS method, which involves minimizing the sum of the squared residuals between the predicted values and the actual values of the dependent variable. This is done by calculating the derivative of the sum of the squared residuals with respect to each regression coefficient, and setting them to zero.\n",
    "\n",
    "Evaluate the goodness of fit: The goodness of fit of the model is evaluated using measures such as the R-squared value, which represents the proportion of variance in the dependent variable that is explained by the independent variables.\n",
    "\n",
    "Make predictions: The final step is to use the estimated regression coefficients to make predictions of the dependent variable based on the values of the independent variables.\n",
    "\n",
    "The OLS algorithm is widely used in linear regression analysis due to its simplicity and effectiveness. However, it assumes that the residuals are normally distributed and have constant variance, and that there is no multicollinearity (high correlation) among the independent variables. If these assumptions are not met, alternative methods such as robust regression or ridge regression may be used.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc556efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 1.0 0.0 1.0 165349.2 136897.8]\n",
      " [1.0 1.0 0.0 0.0 162597.7 151377.59]\n",
      " [1.0 1.0 1.0 0.0 153441.51 101145.55]\n",
      " [1.0 1.0 0.0 1.0 144372.41 118671.85]\n",
      " [1.0 1.0 1.0 0.0 142107.34 91391.77]\n",
      " [1.0 1.0 0.0 1.0 131876.9 99814.71]\n",
      " [1.0 1.0 0.0 0.0 134615.46 147198.87]\n",
      " [1.0 1.0 1.0 0.0 130298.13 145530.06]\n",
      " [1.0 1.0 0.0 1.0 120542.52 148718.95]\n",
      " [1.0 1.0 0.0 0.0 123334.88 108679.17]\n",
      " [1.0 1.0 1.0 0.0 101913.08 110594.11]\n",
      " [1.0 1.0 0.0 0.0 100671.96 91790.61]\n",
      " [1.0 1.0 1.0 0.0 93863.75 127320.38]\n",
      " [1.0 1.0 0.0 0.0 91992.39 135495.07]\n",
      " [1.0 1.0 1.0 0.0 119943.24 156547.42]\n",
      " [1.0 1.0 0.0 1.0 114523.61 122616.84]\n",
      " [1.0 1.0 0.0 0.0 78013.11 121597.55]\n",
      " [1.0 1.0 0.0 1.0 94657.16 145077.58]\n",
      " [1.0 1.0 1.0 0.0 91749.16 114175.79]\n",
      " [1.0 1.0 0.0 1.0 86419.7 153514.11]\n",
      " [1.0 1.0 0.0 0.0 76253.86 113867.3]\n",
      " [1.0 1.0 0.0 1.0 78389.47 153773.43]\n",
      " [1.0 1.0 1.0 0.0 73994.56 122782.75]\n",
      " [1.0 1.0 1.0 0.0 67532.53 105751.03]\n",
      " [1.0 1.0 0.0 1.0 77044.01 99281.34]\n",
      " [1.0 1.0 0.0 0.0 64664.71 139553.16]\n",
      " [1.0 1.0 1.0 0.0 75328.87 144135.98]\n",
      " [1.0 1.0 0.0 1.0 72107.6 127864.55]\n",
      " [1.0 1.0 1.0 0.0 66051.52 182645.56]\n",
      " [1.0 1.0 0.0 1.0 65605.48 153032.06]\n",
      " [1.0 1.0 1.0 0.0 61994.48 115641.28]\n",
      " [1.0 1.0 0.0 1.0 61136.38 152701.92]\n",
      " [1.0 1.0 0.0 0.0 63408.86 129219.61]\n",
      " [1.0 1.0 1.0 0.0 55493.95 103057.49]\n",
      " [1.0 1.0 0.0 0.0 46426.07 157693.92]\n",
      " [1.0 1.0 0.0 1.0 46014.02 85047.44]\n",
      " [1.0 1.0 1.0 0.0 28663.76 127056.21]\n",
      " [1.0 1.0 0.0 0.0 44069.95 51283.14]\n",
      " [1.0 1.0 0.0 1.0 20229.59 65947.93]\n",
      " [1.0 1.0 0.0 0.0 38558.51 82982.09]\n",
      " [1.0 1.0 0.0 0.0 28754.33 118546.05]\n",
      " [1.0 1.0 1.0 0.0 27892.92 84710.77]\n",
      " [1.0 1.0 0.0 0.0 23640.93 96189.63]\n",
      " [1.0 1.0 0.0 1.0 15505.73 127382.3]\n",
      " [1.0 1.0 0.0 0.0 22177.74 154806.14]\n",
      " [1.0 1.0 0.0 1.0 1000.23 124153.04]\n",
      " [1.0 1.0 1.0 0.0 1315.46 115816.21]\n",
      " [1.0 1.0 0.0 0.0 0.0 135426.92]\n",
      " [1.0 1.0 0.0 1.0 542.05 51743.15]\n",
      " [1.0 1.0 0.0 0.0 0.0 116983.8]]\n"
     ]
    }
   ],
   "source": [
    "# step 2\n",
    "X_opt = X[:,[0,1,2,3,4,5]]# its because we remove every column at every step\n",
    "print(X_opt) # dtype is OBJECT try to change dtype to FLOAT\n",
    "X_opt = np.array(X_opt,dtype = float)\n",
    "regressor_OLS = sm.OLS(endog = y , exog = X_opt).fit()# endog is dependent variable exog is independent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "400790f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.948</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.943</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   205.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 19 Apr 2023</td> <th>  Prob (F-statistic):</th> <td>2.90e-28</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>00:31:53</td>     <th>  Log-Likelihood:    </th> <td> -526.75</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1064.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    45</td>      <th>  BIC:               </th> <td>   1073.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>  2.73e+04</td> <td> 3185.530</td> <td>    8.571</td> <td> 0.000</td> <td> 2.09e+04</td> <td> 3.37e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>  2.73e+04</td> <td> 3185.530</td> <td>    8.571</td> <td> 0.000</td> <td> 2.09e+04</td> <td> 3.37e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td> 1091.1075</td> <td> 3377.087</td> <td>    0.323</td> <td> 0.748</td> <td>-5710.695</td> <td> 7892.910</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>  -39.3434</td> <td> 3309.047</td> <td>   -0.012</td> <td> 0.991</td> <td>-6704.106</td> <td> 6625.420</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>    0.8609</td> <td>    0.031</td> <td>   27.665</td> <td> 0.000</td> <td>    0.798</td> <td>    0.924</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>   -0.0527</td> <td>    0.050</td> <td>   -1.045</td> <td> 0.301</td> <td>   -0.154</td> <td>    0.049</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>14.275</td> <th>  Durbin-Watson:     </th> <td>   1.197</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  19.260</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.953</td> <th>  Prob(JB):          </th> <td>6.57e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.369</td> <th>  Cond. No.          </th> <td>7.23e+16</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 2.06e-22. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.948\n",
       "Model:                            OLS   Adj. R-squared:                  0.943\n",
       "Method:                 Least Squares   F-statistic:                     205.0\n",
       "Date:                Wed, 19 Apr 2023   Prob (F-statistic):           2.90e-28\n",
       "Time:                        00:31:53   Log-Likelihood:                -526.75\n",
       "No. Observations:                  50   AIC:                             1064.\n",
       "Df Residuals:                      45   BIC:                             1073.\n",
       "Df Model:                           4                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const        2.73e+04   3185.530      8.571      0.000    2.09e+04    3.37e+04\n",
       "x1           2.73e+04   3185.530      8.571      0.000    2.09e+04    3.37e+04\n",
       "x2          1091.1075   3377.087      0.323      0.748   -5710.695    7892.910\n",
       "x3           -39.3434   3309.047     -0.012      0.991   -6704.106    6625.420\n",
       "x4             0.8609      0.031     27.665      0.000       0.798       0.924\n",
       "x5            -0.0527      0.050     -1.045      0.301      -0.154       0.049\n",
       "==============================================================================\n",
       "Omnibus:                       14.275   Durbin-Watson:                   1.197\n",
       "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               19.260\n",
       "Skew:                          -0.953   Prob(JB):                     6.57e-05\n",
       "Kurtosis:                       5.369   Cond. No.                     7.23e+16\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 2.06e-22. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 3 \n",
    "regressor_OLS.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbc0fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 4\n",
    "X_opt = X[:,[0,1,3,4,5]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
